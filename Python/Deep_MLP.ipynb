{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>homeMSOA</th>\n",
       "      <th>workMSOA</th>\n",
       "      <th>workhome</th>\n",
       "      <th>metro</th>\n",
       "      <th>train</th>\n",
       "      <th>bus</th>\n",
       "      <th>taxi</th>\n",
       "      <th>motorcycle</th>\n",
       "      <th>car</th>\n",
       "      <th>cycle</th>\n",
       "      <th>...</th>\n",
       "      <th>otherqual</th>\n",
       "      <th>centheat</th>\n",
       "      <th>nrooms</th>\n",
       "      <th>wzclass</th>\n",
       "      <th>distance</th>\n",
       "      <th>response</th>\n",
       "      <th>disttrainstn</th>\n",
       "      <th>distcoachstn</th>\n",
       "      <th>distbusstop</th>\n",
       "      <th>distmway</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E02002183</td>\n",
       "      <td>E02002184</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01506</td>\n",
       "      <td>0.069277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.743976</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028852</td>\n",
       "      <td>0.980225</td>\n",
       "      <td>5.9</td>\n",
       "      <td>4</td>\n",
       "      <td>2.684270</td>\n",
       "      <td>0.144439</td>\n",
       "      <td>0.437264</td>\n",
       "      <td>7.354263</td>\n",
       "      <td>0.021183</td>\n",
       "      <td>22.434402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E02002183</td>\n",
       "      <td>E02002185</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.12500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028852</td>\n",
       "      <td>0.980225</td>\n",
       "      <td>5.9</td>\n",
       "      <td>6</td>\n",
       "      <td>7.265346</td>\n",
       "      <td>0.058454</td>\n",
       "      <td>0.270787</td>\n",
       "      <td>7.354263</td>\n",
       "      <td>0.031054</td>\n",
       "      <td>18.741927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E02002183</td>\n",
       "      <td>E02002186</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.011628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.941860</td>\n",
       "      <td>0.011628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028852</td>\n",
       "      <td>0.980225</td>\n",
       "      <td>5.9</td>\n",
       "      <td>6</td>\n",
       "      <td>4.906847</td>\n",
       "      <td>0.229892</td>\n",
       "      <td>1.636134</td>\n",
       "      <td>5.263695</td>\n",
       "      <td>0.483769</td>\n",
       "      <td>23.216051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E02002183</td>\n",
       "      <td>E02002187</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028852</td>\n",
       "      <td>0.980225</td>\n",
       "      <td>5.9</td>\n",
       "      <td>4</td>\n",
       "      <td>6.938770</td>\n",
       "      <td>0.166186</td>\n",
       "      <td>0.935954</td>\n",
       "      <td>6.985427</td>\n",
       "      <td>0.328198</td>\n",
       "      <td>18.379430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E02002183</td>\n",
       "      <td>E02002188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028852</td>\n",
       "      <td>0.980225</td>\n",
       "      <td>5.9</td>\n",
       "      <td>5</td>\n",
       "      <td>4.595884</td>\n",
       "      <td>-0.002455</td>\n",
       "      <td>2.604336</td>\n",
       "      <td>3.576567</td>\n",
       "      <td>0.699492</td>\n",
       "      <td>19.114812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    homeMSOA   workMSOA  workhome  metro    train       bus  taxi  motorcycle  \\\n",
       "0  E02002183  E02002184       0.0    0.0  0.01506  0.069277   0.0         0.0   \n",
       "1  E02002183  E02002185       0.0    0.0  0.12500  0.000000   0.0         0.0   \n",
       "2  E02002183  E02002186       0.0    0.0  0.00000  0.011628   0.0         0.0   \n",
       "3  E02002183  E02002187       0.0    0.0  0.00000  0.000000   0.0         0.0   \n",
       "4  E02002183  E02002188       0.0    0.0  0.00000  0.100000   0.0         0.0   \n",
       "\n",
       "        car     cycle    ...      otherqual  centheat  nrooms  wzclass  \\\n",
       "0  0.743976  0.012048    ...       0.028852  0.980225     5.9        4   \n",
       "1  0.812500  0.000000    ...       0.028852  0.980225     5.9        6   \n",
       "2  0.941860  0.011628    ...       0.028852  0.980225     5.9        6   \n",
       "3  0.916667  0.000000    ...       0.028852  0.980225     5.9        4   \n",
       "4  0.700000  0.000000    ...       0.028852  0.980225     5.9        5   \n",
       "\n",
       "   distance  response  disttrainstn  distcoachstn  distbusstop   distmway  \n",
       "0  2.684270  0.144439      0.437264      7.354263     0.021183  22.434402  \n",
       "1  7.265346  0.058454      0.270787      7.354263     0.031054  18.741927  \n",
       "2  4.906847  0.229892      1.636134      5.263695     0.483769  23.216051  \n",
       "3  6.938770  0.166186      0.935954      6.985427     0.328198  18.379430  \n",
       "4  4.595884 -0.002455      2.604336      3.576567     0.699492  19.114812  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "#os.getcwd()\n",
    "df = pd.read_csv(\"../data/wyflows_w_response_and_geo.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['homeMSOA', 'workMSOA', 'workhome', 'metro', 'train', 'bus', 'taxi',\n",
       "       'motorcycle', 'car', 'cycle', 'walk', 'othertransp', 'npeople', '16-24',\n",
       "       '25-34', '35-49', '50-54', '65-74', '75+', 'female', 'house0carpct',\n",
       "       'house1carpct', 'house2carpct', 'house3carpct', 'house4carpct',\n",
       "       'ppperhect', 'econactivpct', 'econinactivpct', 'vghealth', 'ghealth',\n",
       "       'fhealth', 'bhealth', 'vbhealth', 'white', 'mixed', 'asian', 'black',\n",
       "       'otherethn', 'noqual', 'aptshpqual', 'lev1qual', 'lev2qual', 'lev3qual',\n",
       "       'lev4qual', 'otherqual', 'centheat', 'nrooms', 'wzclass', 'distance',\n",
       "       'response', 'disttrainstn', 'distcoachstn', 'distbusstop', 'distmway'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xfull = df.drop(['homeMSOA','workMSOA','car','response','npeople','workhome','metro','train','bus','taxi','motorcycle','cycle','walk','othertransp'], axis=1).values\n",
    "Yfull = df['car'].values\n",
    "\n",
    "predictors = list(df.drop(['homeMSOA','workMSOA','car','response','npeople','workhome','metro','train','bus','taxi','motorcycle','cycle','walk','othertransp'], axis=1).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DO NOT USE THE FULL DATASET, SPLIT OFF A TRAINING SET BEFORE GOING FURTHER, AND NEVER USE THE FULL SET\n",
    "\n",
    "import math\n",
    "\n",
    "idx = list(range(Xfull.shape[0]))\n",
    "np.random.seed(seed=5)\n",
    "np.random.shuffle(idx)\n",
    "np.random.seed(seed=None)\n",
    "\n",
    "trainsetsize = math.floor(Xfull.shape[0]/2)\n",
    "trainidx = idx[:trainsetsize]\n",
    "\n",
    "X, Y = Xfull[trainidx,:], Yfull[trainidx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['16-24',\n",
       " '25-34',\n",
       " '35-49',\n",
       " '50-54',\n",
       " '65-74',\n",
       " '75+',\n",
       " 'female',\n",
       " 'house0carpct',\n",
       " 'house1carpct',\n",
       " 'house2carpct',\n",
       " 'house3carpct',\n",
       " 'house4carpct',\n",
       " 'ppperhect',\n",
       " 'econactivpct',\n",
       " 'econinactivpct',\n",
       " 'vghealth',\n",
       " 'ghealth',\n",
       " 'fhealth',\n",
       " 'bhealth',\n",
       " 'vbhealth',\n",
       " 'white',\n",
       " 'mixed',\n",
       " 'asian',\n",
       " 'black',\n",
       " 'otherethn',\n",
       " 'noqual',\n",
       " 'aptshpqual',\n",
       " 'lev1qual',\n",
       " 'lev2qual',\n",
       " 'lev3qual',\n",
       " 'lev4qual',\n",
       " 'otherqual',\n",
       " 'centheat',\n",
       " 'nrooms',\n",
       " 'wzclass',\n",
       " 'distance',\n",
       " 'disttrainstn',\n",
       " 'distcoachstn',\n",
       " 'distbusstop',\n",
       " 'distmway']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#cross_valsets = StratifiedKFold(n_splits=2)\n",
    "cross_valsets = KFold(n_splits=10)\n",
    "for train_index, test_index in cross_valsets.split(X, Y):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    Xtrain, Xval = X[train_index], X[test_index]\n",
    "    Ytrain, Yval = Y[train_index], Y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(Xtrain)\n",
    "Xtrain = scaler.transform(Xtrain)\n",
    "Xval = scaler.transform(Xval)\n",
    "\n",
    "# We use X = X_train + X_test for the cross-validation runs, so scale X too (note: X is already a training set)\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import pickle\n",
    "\n",
    "#with open('/home/ilan/Desktop/Homesite_Quote_conversion_Kaggle/X_standard_train.pkl', 'rb') as fb:\n",
    "#    Xtrain = pickle.load(fb)\n",
    "\n",
    "#with open('/home/ilan/Desktop/Homesite_Quote_conversion_Kaggle/X_standard_val.pkl', 'rb') as fb:\n",
    "#    Xtest = pickle.load(fb)\n",
    "    \n",
    "#with open('/home/ilan/Desktop/Homesite_Quote_conversion_Kaggle/Ytrain.pkl', 'rb') as fb:\n",
    "#    Ytrain = pickle.load(fb)\n",
    "    \n",
    "#with open('/home/ilan/Desktop/Homesite_Quote_conversion_Kaggle/Yval.pkl', 'rb') as fb:\n",
    "#    Ytest = pickle.load(fb)\n",
    "    \n",
    "\n",
    "    \n",
    "#with open('/home/ilan/Desktop/BNP_Paribas_Cardif_Kaggle/X_standard_train.pkl', 'rb') as fb:\n",
    "#    Xtrain = pickle.load(fb)\n",
    "\n",
    "#with open('/home/ilan/Desktop/BNP_Paribas_Cardif_Kaggle/X_standard_val.pkl', 'rb') as fb:\n",
    "#    Xtest = pickle.load(fb)\n",
    "    \n",
    "#with open('/home/ilan/Desktop/BNP_Paribas_Cardif_Kaggle/Ytrain.pkl', 'rb') as fb:\n",
    "#    Ytrain = pickle.load(fb)\n",
    "    \n",
    "#with open('/home/ilan/Desktop/BNP_Paribas_Cardif_Kaggle/Yval.pkl', 'rb') as fb:\n",
    "#    Ytest = pickle.load(fb)\n",
    "    \n",
    " \n",
    "    \n",
    "#X = np.concatenate((Xtrain, Xtest), axis=0)\n",
    "#Y = np.concatenate((Ytrain, Ytest), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data set into training, validation, and test sets.  \n",
    "We will not touch the test set at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def TrainValTestSplit(X, Y, fracs_list=[0.7, 0.2, 0.1], shuffle=True, seed=42):\n",
    "    \"\"\"Split the samples x features Numpy array, X, into training, validation, and test sets as\n",
    "    specified in fractions in list fracs_list (e.g. [0.6, 0.2, 0.2])\n",
    "    \n",
    "    Return Numpy arrays:\n",
    "    Xtrain, Ytrain, Xval, Yval, Xtest, Ytest\"\"\"\n",
    "    \n",
    "    assert X.shape[0] == Y.shape[0]\n",
    "    \n",
    "    assert sum(fracs_list) == 1.0\n",
    "    \n",
    "    trainfrac, valfrac, testfrac = fracs_list\n",
    "    idxs = np.arange(X.shape[0])\n",
    "\n",
    "    if shuffle==True:\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(idxs)\n",
    "        np.random.seed(None) # Reset to the default value in case numpy uses the seed later (somewhere below)\n",
    "\n",
    "    trainidx = idxs[:int(trainfrac*len(idxs))]\n",
    "    validx = idxs[int(trainfrac*len(idxs)):int(trainfrac*len(idxs))+int(valfrac*len(idxs))]\n",
    "    testidx = idxs[int(trainfrac*len(idxs))+int(valfrac*len(idxs)):]\n",
    "\n",
    "    #Xtrain, Ytrain = X[trainidx], Y[trainidx]\n",
    "    #Xval, Yval = X[validx], Y[validx]\n",
    "    #Xtest, Ytest = X[testidx], Y[testidx]\n",
    "    #return Xtrain, Ytrain, Xval, Yval, Xtest, Ytest\n",
    "    return trainidx,validx,testidx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "trainidx, validx, testidx = TrainValTestSplit(X, Y, fracs_list=[0.6, 0.2, 0.2])\n",
    "\n",
    "Xtrain, Ytrain = X[trainidx], Y[trainidx]\n",
    "Xval, Yval = X[validx], Y[validx]\n",
    "#Xtest, Ytest = X[testidx], Y[testidx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the training and validation sets, as well as the class labels and their distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24213, 40)\n",
      "(2690, 40)\n"
     ]
    }
   ],
   "source": [
    "print(Xtrain.shape)\n",
    "print(Xval.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose scaling. Standard scaling, minmax scaling, whitening, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perform data whitening (make covariance matrix the identity matrix to remove auto-correlations)\n",
    "def whitening(X):\n",
    "    Xcentred = X - np.mean(X, axis=0)\n",
    "    cov = np.dot(Xcentred.T, Xcentred) / float(Xcentred.shape[0])\n",
    "    U,S,V = np.linalg.svd(cov)\n",
    "    Xrot = np.dot(Xcentred,U)\n",
    "    # Xrot_reduced = np.dot(Xcentred, U[:,:100])\n",
    "    Xwhite = Xrot / np.sqrt(S + 1e-5)\n",
    "    return Xwhite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Check for missing values. Impute if values are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fancyImpute(X_incomplete):\n",
    "    \"\"\"Imputation methods. Options: SimpleFill, DenseKNN, SoftImpute,\n",
    "    IterativeSVD, MICE (ordinal variables only), MatrixFactorization,\n",
    "    NuclearNormMinimization (too slow for large matrices).\"\"\"\n",
    "    \n",
    "    import fancyimpute as fi\n",
    "    #from fancyimpute import (NuclearNormMinimization, BiScaler, DenseKNN)\n",
    "\n",
    "    # rescale both rows and columns to have zero mean and unit variance\n",
    "    biscaler = fi.BiScaler()\n",
    "    X_incomplete_normalized = biscaler.fit_transform(X_incomplete)\n",
    "\n",
    "    # use 3 nearest rows which have a feature to fill in each row's missing features\n",
    "    #solver = fi.DenseKNN(k=3)\n",
    "    solver = fi.SoftImpute()\n",
    "    # Options for solver = NuclearNormMinimization\n",
    "    X_filled_normalized = solver.complete(X_incomplete_normalized)\n",
    "    X_filled = biscaler.inverse_transform(X_filled_normalized)\n",
    "\n",
    "    #mse = ((X_filled[missing_mask] - X[missing_mask]) ** 2).mean()\n",
    "    #print(\"MSE of reconstruction: %f\" % mse)\n",
    "    return X_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(float(Xtrain[np.isnan(Xtrain)].shape[0])/Xtrain.shape[0])\n",
    "print(float(Xval[np.isnan(Xval)].shape[0])/Xval.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if Xtrain.any() == np.nan:\n",
    "    Xtrain = fancyImpute(Xtrain)\n",
    "if Xval.any() == np.nan:\n",
    "    Xval = fancyImpute(Xval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24213, 1)\n",
      "(2690, 1)\n"
     ]
    }
   ],
   "source": [
    "Ytrain = Ytrain.reshape((Ytrain.shape[0], 1))\n",
    "print(Ytrain.shape)\n",
    "Yval = Yval.reshape((Yval.shape[0], 1))\n",
    "print(Yval.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release.  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is disabled, cuDNN 5105)\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "from theano import tensor as T\n",
    "#from theano.printing import debugprint\n",
    "\n",
    "#theano.config.compute_test_value = 'warn' # default is 'off'\n",
    "theano.config.compute_test_value = 'off'\n",
    "#print theano.config.optimizer # default value is 'fast_run'\n",
    "#print theano.config.exception_verbosity # default value is 'low'\n",
    "\n",
    "import LasagneUpdateRules_Feb_2017 as lur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9.0beta1.dev-3343d912717ee85c5c2e0572cfae94581b35e32b\n"
     ]
    }
   ],
   "source": [
    "print(theano.__version__) # T.nnet.relu() activation function available only from Theano 0.7.1 onwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def floatX(array):\n",
    "    \"\"\"Convert array to Numpy array of a type supported by the CPU or GPU, accordingly\"\"\"\n",
    "    return np.asarray(array, dtype=theano.config.floatX)\n",
    "\n",
    "def init_weights(shape, frac=0.01):\n",
    "    \"\"\"Initialize weights randomly. Reduce weight size by factor to keep them closer to zero where gradient is largest\n",
    "    and the learning is fastest at the beginning.\n",
    "    This is Glorot initialisation, with a scaling factor.\"\"\"\n",
    "    #return theano.shared(floatX(frac*np.random.uniform(low=0.0, high=1.0, size=shape), borrow=True)\n",
    "    #return theano.shared(floatX(np.sqrt(2/(shape[0]*shape[1]))*np.random.normal(loc=0, scale=1, size=shape)), borrow=True)\n",
    "    return theano.shared(floatX(np.sqrt(2)*np.random.normal(loc=0, scale=1, size=shape)), borrow=True)\n",
    "\n",
    "def init_bias(shape):\n",
    "    \"\"\"Initialize bias to array of zeros\"\"\"\n",
    "    return theano.shared(value=np.zeros(shape, dtype=theano.config.floatX), borrow=True)\n",
    "\n",
    "def init_ortho(shape):\n",
    "    \"\"\"Initialize weights to an orthogonal matrix. Output matrix is square,\n",
    "    output matrix will only be of the requested dimensions if square\"\"\"\n",
    "    #W = np.random.randn(*shape)\n",
    "    W = np.random.normal(loc=0, scale=1, size=shape)\n",
    "    u, s, v = np.linalg.svd(W)\n",
    "    #return theano.shared(value=np.sqrt(2).astype(theano.config.floatX)*u.astype(theano.config.floatX), borrow=True)\n",
    "    #return theano.shared(value=np.sqrt(2/(shape[0]*shape[1])).astype(theano.config.floatX)*u.astype(theano.config.floatX), borrow=True)\n",
    "    return theano.shared(value=np.sqrt(2).astype(theano.config.floatX)*u.astype(theano.config.floatX), borrow=True)\n",
    "\n",
    "\n",
    "#from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "#from theano.sandbox.cuda.rng_curand import CURAND_RandomStreams as RandomStreams\n",
    "\n",
    "def dropout(state_before, p=0.5):\n",
    "    \"\"\"Retain a fraction p of weights and set the other 1-p to zero\"\"\"\n",
    "    #trng = T.shared_randomstreams.RandomStreams(np.random.randint(1000000, size=1))\n",
    "    #trng = RandomStreams()\n",
    "    trng = T.shared_randomstreams.RandomStreams()\n",
    "    proj = T.cast((state_before * trng.binomial(state_before.shape, p=p, n=1, dtype=state_before.dtype))/p, theano.config.floatX)\n",
    "    return proj\n",
    "\n",
    "def to_onehot(array_column, n_classes=None):\n",
    "    \"\"\"Converts column array of integer labels (starting from 0) into\n",
    "    one-hot encoding\"\"\"\n",
    "    #array_column = np.asarray(array_column).flatten()\n",
    "    if not np.equal(np.mod(array_column.any(), 1), 0):\n",
    "        print(\"Problem: labels are not integers\")\n",
    "    if n_classes == None:\n",
    "        # If no. of classes not given, deduce from data.\n",
    "        # Can fail if not all possible classes found in vector.\n",
    "        n_classes = len(np.unique(array_column))\n",
    "    onehotcol = np.zeros((array_column.shape[0], n_classes))\n",
    "    xmin = array_column.min()\n",
    "    if np.equal(np.mod(array_column.all(), 1), 0) and xmin!=0:\n",
    "        print(\"Labels don't start from 0, shifted them\")\n",
    "        array_column_shifted = array_column-xmin\n",
    "        #print(array_column_shifted)\n",
    "        for i in range(array_column_shifted.shape[0]):\n",
    "            #print(i, array_column_shifted[i])\n",
    "            onehotcol[i, int(array_column_shifted[i])] = 1\n",
    "    else:\n",
    "        for i in range(array_column.shape[0]):\n",
    "            onehotcol[i, int(array_column[i])] = 1\n",
    "    return onehotcol\n",
    "\n",
    "import pickle, os\n",
    "\n",
    "def save_model(parameters, filename):\n",
    "    \"\"\"Serialise Theano shared variables to disk\"\"\"\n",
    "    dirname = 'saved_model'\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)\n",
    "    with open(os.path.join(dirname, filename+'.pkl'), 'wb') as fb:\n",
    "        for p in parameters:\n",
    "            pickle.dump(p.get_value(borrow=True),fb, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to 1-of-N representation for N > 2 class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Ytrain = to_onehot(Ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to GPU-friendly single precision floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtrain = floatX(Xtrain)\n",
    "Ytrain = floatX(Ytrain)\n",
    "\n",
    "Xval = floatX(Xval)\n",
    "Yval = floatX(Yval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert data to theano shared variable type to make optimal usage of GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtraintheano = theano.shared(Xtrain, borrow=True)\n",
    "Ytraintheano = theano.shared(Ytrain, borrow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find greatest common divisor between training and validation sets, to use as batchsize to ensure prediction on full validation set. If this is not important, override batch size manually below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from math import gcd\n",
    "\n",
    "batchsize = gcd(Xtrain.shape[0], Xval.shape[0])\n",
    "\n",
    "print(batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batchsize = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nfeats = Xtrain.shape[1]\n",
    "widhidlay = nfeats # To ensure autoencoder input and output have the same dimensions, as they must."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and train the Stacked Denoising Auto-encoders for unsupervised pre-training of the lower (non-output) layers of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_epochs_pretrain = 100\n",
    "n_pretrain_batches = int(Xtraintheano.get_value(borrow=True).shape[0] / batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = T.matrix(name='X')\n",
    "Y = T.matrix(name='Y')\n",
    "index = T.lscalar()\n",
    "\n",
    "def pretrain_model(X, w_h1_pre, b_h1_pre, b_h2_pre):\n",
    "    X = dropout(X, p=0.5)\n",
    "    h1 = T.nnet.sigmoid(T.dot(X,w_h1_pre) + b_h1_pre)\n",
    "    h2 = T.nnet.sigmoid(T.dot(h1,w_h1_pre.T) + b_h2_pre)\n",
    "    return h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_h1_pre = init_weights((nfeats, widhidlay))\n",
    "b_h1_pre = init_bias((batchsize, widhidlay))\n",
    "b_h2_pre = init_bias((batchsize, widhidlay))\n",
    "\n",
    "Xpred = pretrain_model(X, w_h1_pre, b_h1_pre, b_h2_pre)\n",
    "\n",
    "\n",
    "L1 = T.sum(T.abs_(w_h1_pre))\n",
    "L2 = T.sum(T.sqr(w_h1_pre))\n",
    "\n",
    "cost = T.mean(T.sqr(Xpred-X)) #+ 0.001*L1 + 0.001*L2\n",
    "#cost = T.mean(T.sqr(Xpred-X))\n",
    "\n",
    "params = [w_h1_pre, b_h1_pre, b_h2_pre]\n",
    "\n",
    "#updates = sgd(cost,params,lr=0.01)\n",
    "#updates = lur.sgd(cost,params,learning_rate=0.01)\n",
    "#updates = lur.adagrad(cost, params, learning_rate=0.001)\n",
    "#updates = lur.adadelta(cost,params,learning_rate=0.01)\n",
    "updates = lur.adam(cost,params,learning_rate=0.01)\n",
    "#updates = lur.adamax(cost,params,learning_rate=0.005)\n",
    "#updates = lur.rmsprop(cost,params,learning_rate=0.001)\n",
    "#updates = lur.nesterov_momentum(cost,params,learning_rate=0.001)\n",
    "#cost = -T.mean(T.log(yprobs)*Y)\n",
    "\n",
    "pretrain1 = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=(Xpred,w_h1_pre),\n",
    "        #outputs=(cost,yprobs,err),\n",
    "        updates=updates,\n",
    "        allow_input_downcast=True,\n",
    "        givens={\n",
    "            X: Xtraintheano[index * batchsize : (index + 1) * batchsize]\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs_pretrain):\n",
    "    #print(\"Computing epoch {0} of {1}\".format(epoch+1,n_epochs))\n",
    "    for batchidx in range(n_pretrain_batches):\n",
    "        firsthidlay_weights = pretrain1(batchidx)[1]\n",
    "    if epoch == n_epochs_pretrain-1:\n",
    "        ae_output1 = firsthidlay_weights\n",
    "    #print(firsthidlay_weights[0])\n",
    "\n",
    "ae_outputtheano1 = theano.shared(floatX(ae_output1), borrow=True)\n",
    "#ae_outputtheano1 = theano.shared(floatX(firsthidlay_weights), borrow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -2.32635593e+00  -9.50652659e-01   4.47010174e-02 ...,  -1.38667807e-01\n",
      "    1.85808763e-01  -5.17859720e-02]\n",
      " [  7.75005102e+00   5.16786635e-01  -5.47307730e-01 ...,  -2.05138296e-01\n",
      "   -1.04537591e-01  -5.23345992e-02]\n",
      " [ -2.52789044e+00  -1.42049086e+00  -7.43601546e-02 ...,  -5.31671802e-03\n",
      "   -1.54213890e-01   6.42710149e-01]\n",
      " ..., \n",
      " [ -8.88678208e-02  -1.57653876e-02   7.29265690e-01 ...,  -8.05199802e-01\n",
      "   -2.13273391e-02   4.17980045e-01]\n",
      " [  9.05441269e-02  -9.99894023e-01   6.96894109e-01 ...,  -5.45989370e+00\n",
      "    2.39019934e-02  -1.57952309e+00]\n",
      " [ -8.68406370e-02  -1.36894941e+00  -1.33908555e-01 ...,  -2.94424385e-01\n",
      "    1.03929102e-01  -3.97942811e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(ae_outputtheano1.get_value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_h1_pre = init_ortho((widhidlay, widhidlay))\n",
    "b_h1_pre = init_bias((batchsize, widhidlay))\n",
    "b_h2_pre = init_bias((batchsize, widhidlay))\n",
    "b_h3_pre = init_bias((batchsize, widhidlay))\n",
    "\n",
    "\n",
    "def pretrain_model(X, previous_weights, w_h1_pre, b_h1_pre, b_h2_pre, b_h3_pre):\n",
    "    h1 = T.nnet.sigmoid(T.dot(X,previous_weights) + b_h1_pre)\n",
    "    h1 = dropout(h1, p=0.5)\n",
    "    h2 = T.nnet.sigmoid(T.dot(h1,w_h1_pre) + b_h2_pre)\n",
    "    h3 = T.nnet.sigmoid(T.dot(h2,w_h1_pre.T) + b_h3_pre)\n",
    "    return h3\n",
    "\n",
    "Xpred = pretrain_model(X, ae_outputtheano1, w_h1_pre, b_h1_pre, b_h2_pre, b_h3_pre)\n",
    "\n",
    "L1 = T.sum(T.abs_(w_h1_pre))\n",
    "L2 = T.sum(T.sqr(w_h1_pre))\n",
    "\n",
    "cost = T.mean(T.sqr(Xpred-X)) #+ 0.001*L1 + 0.001*L2\n",
    "#cost = T.mean(T.sqr(Xpred-X))\n",
    "\n",
    "params = [w_h1_pre, b_h1_pre, b_h2_pre, b_h3_pre]\n",
    "\n",
    "#updates = sgd(cost,params,lr=0.01)\n",
    "#updates = lur.sgd(cost,params,learning_rate=0.01)\n",
    "#updates = lur.adagrad(cost, params, learning_rate=0.001)\n",
    "#updates = lur.adadelta(cost,params,learning_rate=0.01)\n",
    "updates = lur.adam(cost,params,learning_rate=0.01)\n",
    "#updates = lur.adamax(cost,params,learning_rate=0.005)\n",
    "#updates = lur.rmsprop(cost,params,learning_rate=0.001)\n",
    "#updates = lur.nesterov_momentum(cost,params,learning_rate=0.001)\n",
    "#cost = -T.mean(T.log(yprobs)*Y)\n",
    "\n",
    "pretrain2 = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=(Xpred,w_h1_pre),\n",
    "        #outputs=(cost,yprobs,err),\n",
    "        updates=updates,\n",
    "        allow_input_downcast=True,\n",
    "        givens={\n",
    "            X: Xtraintheano[index * batchsize : (index + 1) * batchsize]\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs_pretrain):\n",
    "    #print(\"Computing epoch {0} of {1}\".format(epoch+1,n_epochs))\n",
    "    for batchidx in range(n_pretrain_batches):\n",
    "        firsthidlay_weights = pretrain2(batchidx)[1]\n",
    "    if epoch == n_epochs_pretrain-1:\n",
    "        ae_output2 = firsthidlay_weights\n",
    "    #print(firsthidlay_weights[0])\n",
    "\n",
    "ae_outputtheano2 = theano.shared(floatX(ae_output2), borrow=True)\n",
    "#ae_outputtheano1 = theano.shared(floatX(firsthidlay_weights), borrow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.20239402  -0.19285268  -0.80508977 ..., -10.27325726  -2.1361258\n",
      "   -1.37374747]\n",
      " [ -0.45245641   0.18777901   0.05764891 ...,  -3.00755262  -0.09142762\n",
      "   -0.54726589]\n",
      " [-19.24469757  -0.07363039   0.37303171 ...,   1.54923129  -0.23637392\n",
      "   -0.14820921]\n",
      " ..., \n",
      " [ -0.84397352  -0.94770455  -0.45031282 ...,  -0.5962882   -0.48407543\n",
      "   -1.34779239]\n",
      " [  0.06986652  -0.20666374   1.22184551 ...,  -1.73674464  -0.44442856\n",
      "   -6.81022644]\n",
      " [ -0.16104195  -1.57446694   0.13233574 ...,  -0.21707682  -0.18352818\n",
      "    0.31077555]]\n"
     ]
    }
   ],
   "source": [
    "print(ae_outputtheano2.get_value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_h1_pre = init_ortho((widhidlay, widhidlay))\n",
    "b_h1_pre = init_bias((batchsize, widhidlay))\n",
    "b_h2_pre = init_bias((batchsize, widhidlay))\n",
    "b_h3_pre = init_bias((batchsize, widhidlay))\n",
    "b_h4_pre = init_bias((batchsize, widhidlay))\n",
    "\n",
    "\n",
    "def pretrain_model(X, previous_weights1, previous_weights2, w_h1_pre, b_h1_pre, b_h2_pre, b_h3_pre, b_h4_pre):\n",
    "    h1 = T.nnet.sigmoid(T.dot(X,previous_weights1) + b_h1_pre)\n",
    "    h2 = T.nnet.sigmoid(T.dot(h1,previous_weights2) + b_h2_pre)\n",
    "    h2 = dropout(h2, p=0.5)\n",
    "    h3 = T.nnet.sigmoid(T.dot(h2,w_h1_pre) + b_h3_pre)\n",
    "    h4 = T.nnet.sigmoid(T.dot(h3,w_h1_pre.T) + b_h4_pre)\n",
    "    return h4\n",
    "\n",
    "Xpred = pretrain_model(X, ae_outputtheano1, ae_outputtheano2, w_h1_pre, b_h1_pre, b_h2_pre, b_h3_pre, b_h4_pre)\n",
    "\n",
    "L1 = T.sum(T.abs_(w_h1_pre))\n",
    "L2 = T.sum(T.sqr(w_h1_pre))\n",
    "\n",
    "cost = T.mean(T.sqr(Xpred-X)) #+ 0.001*L1 + 0.001*L2\n",
    "\n",
    "params = [w_h1_pre, b_h1_pre, b_h2_pre, b_h3_pre, b_h4_pre]\n",
    "\n",
    "#updates = sgd(cost,params,lr=0.01)\n",
    "#updates = lur.sgd(cost,params,learning_rate=0.01)\n",
    "#updates = lur.adagrad(cost, params, learning_rate=0.001)\n",
    "#updates = lur.adadelta(cost,params,learning_rate=0.01)\n",
    "updates = lur.adam(cost,params,learning_rate=0.01)\n",
    "#updates = lur.adamax(cost,params,learning_rate=0.005)\n",
    "#updates = lur.rmsprop(cost,params,learning_rate=0.001)\n",
    "#updates = lur.nesterov_momentum(cost,params,learning_rate=0.001)\n",
    "#cost = -T.mean(T.log(yprobs)*Y)\n",
    "\n",
    "pretrain3 = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=(Xpred,w_h1_pre),\n",
    "        #outputs=(cost,yprobs,err),\n",
    "        updates=updates,\n",
    "        allow_input_downcast=True,\n",
    "        givens={\n",
    "            X: Xtraintheano[index * batchsize : (index + 1) * batchsize]\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs_pretrain):\n",
    "    #print(\"Computing epoch {0} of {1}\".format(epoch+1,n_epochs))\n",
    "    for batchidx in range(n_pretrain_batches):\n",
    "        firsthidlay_weights = pretrain3(batchidx)[1]\n",
    "    if epoch == n_epochs_pretrain-1:\n",
    "        ae_output3 = firsthidlay_weights\n",
    "    #print(firsthidlay_weights[0])\n",
    "\n",
    "ae_outputtheano3 = theano.shared(floatX(ae_output3), borrow=True)\n",
    "#ae_outputtheano1 = theano.shared(floatX(firsthidlay_weights), borrow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.07127632   0.50816733  -0.1544057  ...,   0.40684357   0.12157594\n",
      "  -14.4142437 ]\n",
      " [ -5.97602844  -1.07428706   1.12373877 ...,   3.51851678   2.86282349\n",
      "   -1.06136286]\n",
      " [  0.1390214   -8.44774342 -13.19944096 ...,  -0.62626725  -0.82545531\n",
      "    0.26210824]\n",
      " ..., \n",
      " [  4.11810541 -20.51300621  -8.53683376 ...,  -3.91026545  -0.16650578\n",
      "   -0.19320154]\n",
      " [  0.09775146  -0.26706454  -2.2201438  ...,  -0.67720497  -0.39735353\n",
      "    0.34576094]\n",
      " [  2.29450202   0.2634345    0.05759998 ...,  -1.54899728  -0.84804535\n",
      "    1.66307998]]\n"
     ]
    }
   ],
   "source": [
    "print(ae_outputtheano3.get_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the neural network itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X = T.matrix(name='X')\n",
    "#Y = T.matrix(name='Y')\n",
    "#index = T.lscalar()\n",
    "\n",
    "dropout_p = 0.5\n",
    "relu_alpha = 0.\n",
    "\n",
    "def model_train(X,w_h1,b_h1,w_h2,b_h2,w_h3,b_h3,w_o,b_o):\n",
    "    h1 = T.nnet.relu(T.dot(X,w_h1) + b_h1, alpha=relu_alpha)\n",
    "    #h1 = T.nnet.sigmoid(T.dot(X,w_h1) + b_h1)\n",
    "    h1 = dropout(h1, p=dropout_p)\n",
    "    h2 = T.nnet.relu(T.dot(h1,w_h2) + b_h2, alpha=relu_alpha)\n",
    "    #h2 = T.nnet.sigmoid(T.dot(h1,w_h2) + b_h2)\n",
    "    h2 = dropout(h2, p=dropout_p)\n",
    "    h3 = T.nnet.relu(T.dot(h2,w_h3) + b_h3, alpha=relu_alpha)\n",
    "    #h3 = T.nnet.sigmoid(T.dot(h2,w_h3) + b_h3)\n",
    "    #h3 = dropout(h3, p=dropout_p)\n",
    "    #o = T.nnet.softmax(T.dot(h3,w_o) + b_o)\n",
    "    #o = T.nnet.sigmoid(T.dot(h3,w_o) + b_o)\n",
    "    o = T.dot(h3,w_o) + b_o\n",
    "    return o\n",
    "\n",
    "def model_test(X,w_h1,b_h1,w_h2,b_h2,w_h3,b_h3,w_o,b_o):\n",
    "    h1 = T.nnet.relu(T.dot(X,w_h1) + b_h1, alpha=relu_alpha)\n",
    "    #h1 = T.nnet.sigmoid(T.dot(X,w_h1) + b_h1)\n",
    "    #h1 = dropout(h1, p=dropout_p)\n",
    "    h2 = T.nnet.relu(T.dot(h1,w_h2) + b_h2, alpha=relu_alpha)\n",
    "    #h2 = T.nnet.sigmoid(T.dot(h1,w_h2) + b_h2)\n",
    "    #h2 = dropout(h2, p=dropout_p)\n",
    "    h3 = T.nnet.relu(T.dot(h2,w_h3) + b_h3, alpha=relu_alpha)\n",
    "    #h3 = T.nnet.sigmoid(T.dot(h2,w_h3) + b_h3)\n",
    "    #h3 = dropout(h3, p=dropout_p)\n",
    "    #o = T.nnet.softmax(T.dot(h3,w_o) + b_o)\n",
    "    #o = T.nnet.sigmoid(T.dot(h3,w_o) + b_o)\n",
    "    o = T.dot(h3,w_o) + b_o\n",
    "    return o\n",
    "\n",
    "\n",
    "nfeats = Xtrain.shape[1]\n",
    "widhidlay = nfeats\n",
    "nclasses = 1\n",
    "#nclasses = len(np.unique(Ytrain))  # Use 1 for binary classification or regression\n",
    "\n",
    "# 3 hidden layers, initialise the weights and biases\n",
    "#w_h1 = init_weights((nfeats, widhidlay))\n",
    "w_h1 = ae_outputtheano1\n",
    "b_h1 = init_bias((batchsize, widhidlay))\n",
    "#w_h2 = init_ortho((widhidlay, widhidlay))\n",
    "w_h2 = ae_outputtheano2\n",
    "b_h2 = init_bias((batchsize, widhidlay))\n",
    "#w_h3 = init_ortho((widhidlay, widhidlay))\n",
    "w_h3 = ae_outputtheano3\n",
    "b_h3 = init_bias((batchsize, widhidlay))\n",
    "w_o = init_weights((widhidlay, nclasses))\n",
    "b_o = init_bias((batchsize, nclasses))\n",
    "\n",
    "# Runs the NN and outputs the result from its output node\n",
    "#yprobs_train = model_train(X, w_h1, b_h1, w_h2, b_h2, w_h3, b_h3, w_o, b_o)\n",
    "#yprobs_test = model_test(X, w_h1, b_h1, w_h2, b_h2, w_h3, b_h3, w_o, b_o)\n",
    "ypred_train = model_train(X, w_h1, b_h1, w_h2, b_h2, w_h3, b_h3, w_o, b_o)\n",
    "ypred_test = model_test(X, w_h1, b_h1, w_h2, b_h2, w_h3, b_h3, w_o, b_o)\n",
    "\n",
    "\n",
    "# Predicted label is 1 if its probability is >=0.5, and 0 otherwise.\n",
    "#ypred = (yprobs >= 0.5)\n",
    "\n",
    "\n",
    "#ypred_train = T.argmax(yprobs_train)\n",
    "#ypred_test = T.argmax(yprobs_test)\n",
    "\n",
    "#ypred_train = T.switch(T.lt(yprobs_train, 0.5), 0, 1)\n",
    "#ypred_test = T.switch(T.lt(yprobs_test, 0.5), 0, 1)\n",
    "\n",
    "#ypred_train = T.switch(T.lt(yprobs_train, 0.07), 0, 1)\n",
    "#ypred_test = T.switch(T.lt(yprobs_test, 0.07), 0, 1)\n",
    "\n",
    "#err = T.mean(T.neq(ypred, Y))\n",
    "\n",
    "params = [w_h1, b_h1, w_h2, b_h2, w_h3, b_h3, w_o, b_o]\n",
    "\n",
    "\n",
    "#L1 = T.sum(T.abs_(w_h2)) + T.sum(T.abs_(w_h3)) \n",
    "#L2 = T.sum(T.sqr(w_h2)) + T.sum(T.sqr(w_h3)) \n",
    "\n",
    "L1 = T.sum(T.abs_(w_h2)) + T.sum(T.abs_(w_h3)) + T.sum(T.abs_(w_h1)) + T.sum(T.abs_(w_o))\n",
    "L2 = T.sum(T.sqr(w_h2)) + T.sum(T.sqr(w_h3)) + T.sum(T.sqr(w_h1)) + T.sum(T.sqr(w_o))\n",
    "\n",
    "    \n",
    "#cost = T.mean(T.nnet.categorical_crossentropy(yprobs_train, Y)) + 0.01*L1 + 0.01*L2\n",
    "#cost = T.mean(T.nnet.binary_crossentropy(yprobs_train, Y)) + 0.001*L1 + 0.001*L2\n",
    "#cost = -(Y*T.log(yprobs_train) + (1-Y)*T.log(1-yprobs_train)).mean() + 0.001*L1 + 0.001*L2\n",
    "#cost = T.mean(T.nnet.binary_crossentropy(yprobs, Y)) + 0.01*L2\n",
    "cost = T.mean(T.sqr(ypred_train-Y)) #+ 0.01*L1 + 0.01*L2\n",
    "#cost = T.mean(T.sqr(yprobs_train-Y))\n",
    "\n",
    "#updates = lur.sgd(cost,params,learning_rate=0.001)\n",
    "#updates = lur.adagrad(cost, params, learning_rate=0.01)\n",
    "#updates = lur.adadelta(cost,params,learning_rate=0.01)\n",
    "updates = lur.adam(cost,params,learning_rate=0.01)\n",
    "#updates = lur.adamax(cost,params,learning_rate=0.01)\n",
    "#updates = lur.rmsprop(cost,params,learning_rate=0.001)\n",
    "#updates = lur.nesterov_momentum(cost,params,learning_rate=0.01)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=cost,\n",
    "        #outputs=(cost,yprobs,err),\n",
    "        updates=updates,\n",
    "        allow_input_downcast=True,\n",
    "        givens={\n",
    "            X: Xtraintheano[index * batchsize : (index + 1) * batchsize],\n",
    "            Y: Ytraintheano[index * batchsize : (index + 1) * batchsize]\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "Xvaltheano = theano.shared(Xval, borrow=True)\n",
    "Yvaltheano = theano.shared(Yval, borrow=True)\n",
    "\n",
    "predict = theano.function(\n",
    "        inputs=[index],\n",
    "        #outputs=yprobs,\n",
    "        #outputs=(yprobs,ypred),\n",
    "        #outputs=(cost,yprobs,err),\n",
    "        outputs=ypred_test,\n",
    "        #outputs=(yprobs_test,ypred_test),\n",
    "        allow_input_downcast=True,\n",
    "        givens={\n",
    "            X: Xvaltheano[index * batchsize : (index + 1) * batchsize]\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAF5CAYAAAC7nq8lAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiIiIgVQMCUiIiJSgP8P3gp3P7UshUgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4215148898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "from scipy.stats import mode\n",
    "\n",
    "n_train_batches = int(Xtraintheano.get_value(borrow=True).shape[0] / batchsize)\n",
    "n_test_batches = int(Xvaltheano.get_value(borrow=True).shape[0] / batchsize)\n",
    "\n",
    "n_epochs = 1000\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "start = time.time()\n",
    "epochs = []\n",
    "costs = []\n",
    "errors = []\n",
    "#randomerror = 1 - 1.0/len(np.unique(Yval))\n",
    "#randomerror = np.mean()\n",
    "#mostcommonerror = np.mean(Yval != mode(Yval)[0][0]) #error from always predicting most common class\n",
    "for epoch in range(n_epochs):\n",
    "    epochstart = time.time()\n",
    "    ypreds = np.empty((batchsize*n_test_batches, 1), dtype=float)\n",
    "    #yprobs = np.empty((batchsize*n_test_batches, 1), dtype=float)\n",
    "    for batchidx in range(n_train_batches):\n",
    "        batchcost = train(batchidx)\n",
    "        if batchcost == np.nan:\n",
    "            print(\"NaN cost. Exiting. Try lowering the learning rate.\")\n",
    "            break\n",
    "    for batchidx in range(n_test_batches):\n",
    "        #batchyprob, batchypred = predict(batchidx)\n",
    "        batchypred = predict(batchidx)\n",
    "        #yprobs[batchsize*batchidx:batchsize*(batchidx+1)] = batchyprob\n",
    "        ypreds[batchsize*batchidx:batchsize*(batchidx+1)] = batchypred\n",
    "    #print(\"Epoch: {0}, runtime: {1}, cost: {2}\".format(epoch, time.time()-epochstart, batchcost))\n",
    "    epochs.append(epoch)\n",
    "    costs.append(batchcost)\n",
    "    \n",
    "    epochYtest = Yval[:ypreds.shape[0]]\n",
    "    #if epochYtest.shape[1]>1: #If Yval is in one-hot form change back to single int to compare with ypred\n",
    "    #    epochYtest = np.argmax(epochYtest, axis=1)\n",
    "    \n",
    "    epochErr = np.mean((epochYtest - ypreds)**2)\n",
    "    #epochAcc = np.mean(epochYtest == ypreds)\n",
    "    #epochErr = np.mean(epochYtest != ypreds)\n",
    "    \n",
    "    errors.append(epochErr)\n",
    "    \n",
    "    randomerror = np.mean((epochYtest - np.mean(Yval))**2)\n",
    "    \n",
    "    #if epochErr < randomerror and epochErr < mostcommonerror:\n",
    "    #    save_model(params, 'model-epoch_'+str(epoch))\n",
    "    if (epoch+1)%(n_epochs/10)==0:\n",
    "        # If error drops precipitously at the beginning the y-scale of the plots is large and ruins plots\n",
    "        # remove first epochs to avoid this\n",
    "        epochsplot = epochs[150:]\n",
    "        costsplot = costs[150:]\n",
    "        errorsplot = errors[150:]\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(epochsplot,costsplot, 'b-')\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(\"Cost\")\n",
    "        for tl in ax.get_yticklabels():\n",
    "            tl.set_color('b')\n",
    "\n",
    "        ax1 = ax.twinx()\n",
    "        ax1.plot(epochsplot, errorsplot, 'r-')\n",
    "        ax1.set_ylabel(\"Error\")\n",
    "        ax1.axhline(randomerror, color='r', linestyle='dashed')\n",
    "        #ax1.axhline(mostcommonerror, color='r', linestyle='dotted')\n",
    "        for tl in ax1.get_yticklabels():\n",
    "            tl.set_color('r')    \n",
    "\n",
    "        epochtime = time.time()-epochstart\n",
    "        print(\"Epoch runtime: {0:.3g}s\".format(epochtime), end=\"\\r\")\n",
    "\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "    \n",
    "#    if len(errors) >= 40: # Run at least for 40 epochs\n",
    "#        temperrs = np.asarray(errors)\n",
    "#        if epochErr >= temperrs[-30] - temperrs[-30:].std():\n",
    "#            break # If error hasn't improved by 1 SD in 30 epochs, stop\n",
    "    \n",
    "    #print(\"Epoch: {0}\\tAccuracy: {1:.4g}\\tError: {2:.3g}\\tCost: {3:.6g}\\tRuntime: {4:.3g}\".format(epoch, epochAcc, epochErr, batchcost, epochtime))\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Training took {0:.3g} seconds.\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect output: predicted probabilities and final weight matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(yprobs[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.16370356, -3.98086786, -1.98386347, ..., -0.5157361 ,\n",
       "        -0.54091287,  0.11813538],\n",
       "       [ 3.33419657, -0.04839806, -2.04159522, ..., -3.0195787 ,\n",
       "         0.46194857, -0.69820392],\n",
       "       [-4.07596397, -2.72196841, -0.70420104, ..., -2.61074209,\n",
       "         0.78090554,  0.12692203],\n",
       "       ..., \n",
       "       [ 0.38967064,  2.05850053,  0.44588459, ..., -1.32389522,\n",
       "        -1.79329705, -0.98731303],\n",
       "       [-0.81393236,  0.50369245, -0.57153141, ..., -5.82378006,\n",
       "         1.74907911, -3.99408174],\n",
       "       [-1.00369275,  0.93589145,  0.4778291 , ..., -0.53635001,\n",
       "        -0.11899801, -4.41823578]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_h1.get_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -1.51878476,  -0.91261703,  -1.96008146, ..., -10.50990105,\n",
       "         -3.14269519,  -1.60460389],\n",
       "       [ -2.85292697,  -0.93355691,  -2.38712955, ...,  -3.50388861,\n",
       "         -2.75200105,  -0.6018731 ],\n",
       "       [-19.04048347,  -1.00767517,  -1.42935383, ...,  -0.33378032,\n",
       "         -1.16619968,  -0.73847443],\n",
       "       ..., \n",
       "       [ -4.35225964,  -2.49608421,  -2.67564964, ...,  -1.12343395,\n",
       "         -2.30342722,  -1.38561189],\n",
       "       [ -3.05263925,  -0.86479616,  -0.2090368 , ...,  -1.85978687,\n",
       "         -0.73839927,  -6.79955149],\n",
       "       [ -2.70977736,  -2.39477587,  -1.26831174, ...,  -0.56545234,\n",
       "         -2.02744412,  -0.36778873]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_h2.get_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -3.14625233e-01,  -1.69071585e-01,  -2.89094061e-01, ...,\n",
       "         -9.95607749e-02,  -2.26649597e-01,  -1.41182232e+01],\n",
       "       [ -6.01114464e+00,  -1.39945102e+00,  -1.89159930e-01, ...,\n",
       "          1.04200554e+00,   1.59485567e+00,  -1.49226081e+00],\n",
       "       [ -4.95660067e-01,  -7.69555044e+00,  -1.10553761e+01, ...,\n",
       "         -2.84150302e-01,  -5.42025924e-01,  -8.34790468e-01],\n",
       "       ..., \n",
       "       [  3.16117406e+00,  -1.98365154e+01,  -9.61777306e+00, ...,\n",
       "         -6.00148726e+00,  -2.15854093e-01,  -5.69307566e-01],\n",
       "       [ -5.25342882e-01,  -5.04171550e-01,  -2.00503421e+00, ...,\n",
       "         -4.76541340e-01,  -1.88369125e-01,   4.91034687e-02],\n",
       "       [  2.29792142e+00,  -6.06503904e-01,  -5.75307012e-01, ...,\n",
       "         -1.71857607e+00,  -1.04735899e+00,   1.97401140e-02]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_h3.get_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -1.11938155e+00],\n",
       "       [ -4.11969870e-01],\n",
       "       [ -1.05107687e-01],\n",
       "       [ -6.17304087e-01],\n",
       "       [ -8.87203932e-01],\n",
       "       [ -1.11714692e-03],\n",
       "       [  1.19324252e-01],\n",
       "       [ -1.01136707e-01],\n",
       "       [  5.42461991e-01],\n",
       "       [  5.86019933e-01],\n",
       "       [ -7.23948002e-01],\n",
       "       [  1.12025686e-01],\n",
       "       [  5.19929588e-01],\n",
       "       [ -5.60998857e-01],\n",
       "       [ -7.41516531e-01],\n",
       "       [ -7.78545082e-01],\n",
       "       [ -4.71046893e-03],\n",
       "       [ -3.17572534e-01],\n",
       "       [ -8.34929705e-01],\n",
       "       [ -1.08599886e-01],\n",
       "       [ -5.69290638e-01],\n",
       "       [  1.69950247e-01],\n",
       "       [  2.38008544e-01],\n",
       "       [  3.24593723e-01],\n",
       "       [  1.00301411e-02],\n",
       "       [ -8.64978433e-01],\n",
       "       [  4.08147991e-01],\n",
       "       [  8.01246703e-01],\n",
       "       [  6.58710659e-01],\n",
       "       [  2.47755185e-01],\n",
       "       [  5.45977056e-01],\n",
       "       [ -5.88298380e-01],\n",
       "       [ -8.58817846e-02],\n",
       "       [  6.14628434e-01],\n",
       "       [ -3.37409049e-01],\n",
       "       [ -6.17715940e-02],\n",
       "       [ -1.57738656e-01],\n",
       "       [  6.69074506e-02],\n",
       "       [  1.98442623e-01],\n",
       "       [ -8.11579406e-01]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_o.get_value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse the final performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum error at epoch 963: 0.0727 (Accuracy: 0.9273)\n"
     ]
    }
   ],
   "source": [
    "errors = np.asarray(errors)\n",
    "print(\"Minimum error at epoch {0}: {1:.3g} (Accuracy: {2:.4g})\".format(errors.argmin(), errors[errors.argmin()], 1-errors[errors.argmin()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "n_test_batches = int(Xvaltheano.get_value(borrow=True).shape[0] / batchsize)\n",
    "\n",
    "results = np.empty((batchsize*n_test_batches, 1),dtype=int)\n",
    "#resultprobs = np.empty((batchsize*n_test_batches, 1),dtype=float)\n",
    "for batchidx in range(n_test_batches):\n",
    "    #resultprobs[batchsize*batchidx:batchsize*(batchidx+1)] = predict(batchidx)[0]\n",
    "    #results[batchsize*batchidx:batchsize*(batchidx+1)] = predict(batchidx)[1]\n",
    "    results[batchsize*batchidx:batchsize*(batchidx+1)] = predict(batchidx)[0]\n",
    "    \n",
    "results = np.asarray(results, dtype=int)\n",
    "results = results.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "#Yval_testing = Yval_testing.reshape(Yval_testing.shape[0],)\n",
    "\n",
    "Yval_testing = Yval[:results.shape[0]]\n",
    "Xval_testing = Xval[:results.shape[0]]\n",
    "\n",
    "#if Yval_testing.shape[1] > 0:\n",
    "#    Yval_testing = np.argmax(Yval_testing, axis=1\n",
    "Yval_testing = Yval_testing.flatten()\n",
    "Yval_testing = np.asarray(Yval_testing, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#np.unique(ypreds)\n",
    "yprobs[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(Yval_testing, yprobs[:Yval_testing.shape[0]], pos_label=1)\n",
    "roc_auc = roc_auc_score(Yval_testing, yprobs[:Yval_testing.shape[0]])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def find_nearest(array,value):\n",
    "    idx = (np.abs(array-value)).argmin()\n",
    "    return array[idx]\n",
    "\n",
    "\n",
    "thresholds[np.where(fpr == find_nearest(fpr,0.5))]\n",
    "#fpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "NN_accuracy = np.mean(Yval_testing == results)\n",
    "\n",
    "print(\"Accuracy: {0:.2g}\".format(NN_accuracy))\n",
    "print(\"\\n(Accuracy of random guesses: {0:.2g})\".format(1.0/len(np.unique(epochYtest))))\n",
    "print(\"\\n(Accuracy of predicting dominant class for all: {0:.2g})\".format(np.mean(Yval == mode(Yval)[0][0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print(results[:30])\n",
    "print(Yval_testing[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "plt.figure(1)\n",
    "plt.subplot(211)\n",
    "plt.hist(results)\n",
    "plt.subplot(212)\n",
    "plt.hist(Yval_testing)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Function to plot the confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, target_names):\n",
    "    ncls = len(set(y_true))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    \n",
    "    # Normalise each row to its total (i.e. normalise by the number of samples in each class)\n",
    "    cm_normalised = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    fig = plt.figure(figsize=(12,12))\n",
    "    plt.clf()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_aspect(1)\n",
    "    #plt.cm.Blues\n",
    "    #res = ax.imshow(np.array(cm_normalised), cmap=plt.cm.jet, \n",
    "    #                interpolation='nearest')\n",
    "    res = ax.pcolor(np.array(cm_normalised))\n",
    "\n",
    "\n",
    "    height = cm.shape[0]\n",
    "    width = cm.shape[1]\n",
    "\n",
    "    for x in range(ncls):\n",
    "        for y in range(ncls):\n",
    "            ax.annotate(str(cm[x][y]), xy=(y+0.5, x+0.5), \n",
    "                        horizontalalignment='center',\n",
    "                        verticalalignment='center',\n",
    "                        fontsize=20,color='w')\n",
    "\n",
    "    cb = fig.colorbar(res,fraction=0.046, pad=0.04)\n",
    "\n",
    "    #plt.imshow(cm_normalised, cmap='jet')\n",
    "    \n",
    "    plt.xticks(np.arange(ncls)+0.5, target_names, fontsize=16,rotation='vertical')\n",
    "    plt.yticks(np.arange(ncls)+0.5, target_names, fontsize=16)\n",
    "    #plt.tight_layout()\n",
    "    plt.xlabel(\"Predicted label\", fontsize=20)\n",
    "    plt.ylabel(\"True label\", fontsize=20)\n",
    "    plt.title(\"Confusion matrix\", fontsize=22)\n",
    "    fig=plt.figure()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#target_names = ['Setosa', 'Versicolour','Virginica']\n",
    "#target_names = [str(i) for i in np.unique(np.concatenate((Ytrain, Yval_testing), axis=0))]\n",
    "\n",
    "target_names = [\"0\",\"1\"]\n",
    "\n",
    "print(confusion_matrix(Yval_testing, results))\n",
    "plot_confusion_matrix(Yval_testing, results, target_names=target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(Yval_testing, results, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark agains other similar classifiers, possibly one simple and similar one as a baseline, and one usually performant one (e.g. XGBoost) as a benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Concatenate training and validation sets and undo one-hot encoding for sklearn classifiers to run CV\n",
    "Xsklearn = np.concatenate((Xtrain,Xval), axis=0)\n",
    "\n",
    "if Ytrain.shape[1] > 0:\n",
    "    Ysklearn = np.concatenate((np.argmax(Ytrain, axis=1), Yval.flatten()),axis=0)\n",
    "else:\n",
    "    np.concatenate((Ytrain,Yval), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.Perceptron()\n",
    "\n",
    "score = cross_val_score(clf, Xsklearn, Ysklearn, cv=10)\n",
    "\n",
    "print(score)\n",
    "print(score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from sklearn import linear_model\n",
    "#clf = linear_model.ElasticNet()\n",
    "clf = linear_model.LogisticRegression(penalty='l2', n_jobs=4)\n",
    "\n",
    "score = cross_val_score(clf, Xsklearn, Ysklearn, cv=10)\n",
    "\n",
    "print(score)\n",
    "print(score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# If you have XGBoost installed you can benchmark against it as well:\n",
    "\n",
    "# For description of XGBoost algorithm see:\n",
    "#  http://xgboost.readthedocs.io/en/latest/model.html\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "clf = XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05)\n",
    "#clf = XGBClassifier()\n",
    "score = cross_val_score(clf, Xsklearn, Ysklearn, cv=10)\n",
    "\n",
    "print(score)\n",
    "print(score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Just by predicting zero everywhere we would get this fraction right\n",
    "Yval_testing[Yval_testing==0].shape[0]/float(Yval_testing.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply a shallow (desired for interpretability, see later) decision tree to the data, as a benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(max_depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "score = cross_val_score(clf, Xsklearn, Ysklearn, cv=3)\n",
    "\n",
    "print(score)\n",
    "print(score.mean())\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(max_depth=2)\n",
    "clf_data_to_target = clf.fit(Xsklearn, Ysklearn)\n",
    "tree.export_graphviz(clf_data_to_target, out_file='tree.dot')\n",
    "\n",
    "# sudo apt-get install graphviz\n",
    "!dot -Tpng tree.dot -o tree.png\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(filename='tree.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use a decision tree to fit the NN results from the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(max_depth=2)\n",
    "\n",
    "Xval_unscaled = scaler_val.inverse_transform(Xval)\n",
    "Xval_unscaled = Xval_unscaled[:results.shape[0]]\n",
    "\n",
    "clf_data_to_NN = clf.fit(Xval_unscaled, results)\n",
    "tree.export_graphviz(clf_data_to_NN, out_file='NN_tree.dot')\n",
    "\n",
    "!dot -Tpng NN_tree.dot -o NN_tree.png\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(filename='NN_tree.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#list(zip(iris.data, iris.target))\n",
    "#np.concatenate((iris.data[validx], iris.target[validx].reshape(len(validx),1)), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate of the accuracy with which the decision tree replicates the NN:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Try a measure of inter-rater agreement such as (weighted Cohen's kappa)\n",
    "\n",
    "#import ml_metrics\n",
    "#print(ml_metrics.quadratic_weighted_kappa(clf_data_to_NN.predict(Xval_unscaled), results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print(np.mean(clf_data_to_NN.predict(Xval_unscaled) == results))\n",
    "\n",
    "score = cross_val_score(clf_data_to_NN, Xval_unscaled, results, cv=3)\n",
    "\n",
    "print(score)\n",
    "print(score.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to ensure we have an \"explanation\" for all predictions we can predict on the data directly from the decision tree trained on the NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print(\"Accuracy of predicting from decision tree trained on NN: {0:.2g}\".format(np.mean(clf_data_to_NN.predict(Xval_unscaled) == Yval)))\n",
    "print(\"\\n(c.f. Accuracy of NN itself: {0:.2g})\".format(NN_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise, we can predict from the NN itself, in which case we will have no loss of accuracy, but we will only have an \"explanation\" for the cases where the NN prediction and that of the decision tree prediction match.  \n",
    "  In this case we would not have an \"explanation\" for this fraction of samples from the validation set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print(np.mean(clf_data_to_NN.predict(Xval_unscaled) != results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note that the cross-validation accuracy of the NN-mimicking decision tree against the NN is perhaps a better estimate for the fraction of samples for which we would have an \"explanation\", of sorts.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, assuming the NN has better performance on the data set than a shallow decision tree -which is not true in the toy example above- (we assume this as otherwise using a NN would not make sense and one could directly use a more interpretable classifier which outperforms a NN), there is therefore always a trade-off, and one can choose between:  \n",
    "- Have an explanation for all samples, at the cost of taking a hit in predictive accuracy (i.e. predict from the decision tree trained on the NN), or\n",
    "- Retain the full accuracy of the NN, but accept a certain fraction of samples for whose prediction we will have no explanation (i.e. predict directly from the NN, using the decision tree's graph to \"explain\" the classification if the prediction of the NN and the decision tree trained on the NN coincide)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tentative thoughts on this approach\n",
    "\n",
    "As mentioned, this method only makes sense if a NN is the dominant classifier, so in the following we assume this to be the case throughout.  \n",
    "  \n",
    "As a thought experiment, consider the limit in which the NN tends to perfect accuracy (or optimises whatever other target metric is being used). In this case, as far as the (now optimised) performance metric is concerned, the NN is indistinguishable from the generative process underlying the data, and the difference between training the decision tree on the NN, or training it directly on the data, vanishes.  \n",
    "  \n",
    "Therefore, the accuracy obtained by training the decision tree (of the desired interpretable depth, etc) directly on the data appears to be the upper bound on performance that can be expected from a decision tree trained on the corresponding NN trained on the same data, with the bound being saturated as the NN's performance tends to perfect.\n",
    "  \n",
    "Tentative conclusion: **One cannot expect better interpretable performance by training an interpretable classifier on a superior (but uninterpretable) classifier (e.g. a NN), than that obtained by training the interpretable classifier directly on the data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above reasoning is not flawed (which it may very well be!), then there would be no benefit to training an interpretable classifier on a NN and using the simpler classifier to predict. Using the NN to predict, and using the interpretable classifier only to provide a plausible rationale where their predictions match still makes sense, as long as one is willing to accept the fraction of samples for which the NN and the interpretable classifier differ, and therefore no explanation can be given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
